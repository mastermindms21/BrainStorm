<html>
<body>


<h1>ReLU - Smart Activation Functions</h1>
<quote>
Rectified Linear Units, or ReLUs, are a type of activation function 
that are linear in the positive dimension, but zero in the negative dimension.
</quote>




<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>f</mi>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mi>x</mi>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>=</mo>
  <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mi>x</mi>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
</math>






</body>
</html>